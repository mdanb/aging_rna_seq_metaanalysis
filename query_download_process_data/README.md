# Query the NCBI databases, download, and process the data
The relevant scripts for this stage of the workflow are in the directory `query_download_process_data` in the container, so make sure you `cd` into there. 

**1. Query the NCBI database to obtain aging RNA-seq gene expression studies and important metadata associated with them**
    
  Run `python3 get_accession_list.py`, which will query NCBI databases in three ways:
  - Query the GEO database for any RNA-seq study whose abstract mentions one of the following terms: ['lifespan', 'aging', 'life span', 'longevity', 'senescence', 'ageing', 'longlived', 'shortlived', 'long-lived', 'short-lived', 'stress']. Query the SRA database to obtain metadata associated with these studies. 
  - Query the GEO database for any RNA-seq study whose abstract mentions a list of aging genes which can be found in the file `genage_all_genes.txt`. This list was obtained from the [GenAge database](https://genomics.senescence.info/genes/search.php?organism=Caenorhabditis+elegans&show=4), Build 20. Query the SRA database to obtain metadata associated with these studies. 
  - Query the SRA database by BioProject ID (which you can think of as IDs for different studies) to obtain metadata for relevant studies that we know of that were missed by the above two options (just one study). 
 
The important files created after querying are `keywords_metadata.p`, `genage_genes_metadata.p`, `manually_fetched_metadata.p`, as well as `all_bioprojects.txt`. The first three contain the metadata associated with SRA accession numbers (IDs for gene expression data samples) for all replicates in all the studies that were fetched. The last file contains all BioProject IDs. At this point, using the latter file, we perform a quick manual check of the different studies (which is inevitable at this point) to filter out the irrelevant ones (see `relevant_bioprojects` in `get_accession_list.py`)

Next, we manually obtain SRA Run Tables (visit [this link](https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA491191&o=acc_s%3Aa) to download an example of an SRA Run Table by clicking `MetaData` under `Download`) for all studies. At the time when we performed our analysis, [it seemed like](https://bioinformatics.stackexchange.com/questions/16082/programmatically-retrieve-metadata-from-sra-run-selector) there was no way to programmatically obtain these files. Anyway, these files can be found in the directory `raw_sra_metadata`. Using these files, the different samples are expert labeled as "longlived", "shortlived", or "normal". We throw out samples that we were not able to obtain labels for (see `samples_to_throw_out` in `get_accession_list.py`). When available, we also obtain age information for the samples from these files. Otherwise, we look for age data from the research papers. If we do not find age information, we consider the samples to be day 1 adults, which is the mode age. Label and age information can be found in `../common_datastore` in `labels.csv` and `age.csv` respectively. Finally, the file `accession_list.txt` is created, which contains SRA run IDs for samples that we want to download, along with their BioProject IDs and library layout (single vs paired end sequencing). This file will be used as input to the download stage of the pipeline.  

**2. Download the SRA runs (in the form of FASTQ files) and map them to a reference transcriptome**

To download and process the SRA runs in `accession_list.txt`, and map the FASTQ files to a reference transcriptome, we provide a suite of bash scripts to completely automate the process: `first_queue.sh`, `first_queue_cleanup.sh`, `second_queue.sh`, `second_queue_cleanup.sh`, `run_second_queue_cleanup.sh`, and `monitor_disk_space.sh` (the only one that you need to run is the last one, the others are called by it). The basic idea is that we want to download many FASTQ files. However, these files are huge (around 10 GB each), and we need to download 1241 of them (number of lines in `accession_list.txt`). Actually, the disk space requirement for each entry in `accession_list.txt` can reach up to around 40 GB, not just 10 GB. So a reasonable upper bound for the amount of disk space we need is 40GB * 1241 â‰ˆ 50 TB. That's a lot of disk space, and this means that we obviously cannot download the whole data, then map them to a reference transcriptome (for efficiency reasons, we wouldn't want to anyway). Instead, we take the following approach: download the data in batches of 6 (or any other reasonably small number of) accession entries, zip the FASTQ files, and begin mapping them to a reference transcriptome. When we start mapping to a reference transcriptome, we begin downloading the next batch of 6 files. So now we're downloading the next batch of files, and mapping the current batch in parallel. The moment we're done mapping the FASTQ files, we delete them to free up disk space. Note that the output of mapping the FASTQ files occupies a negligeable amount of disk space, but that output is really what we care about. And this process repeats until we have all files downloaded and processed. Now, in order to begin the process, you'll have to modify `monitor_disk_space.sh` to suit the amount of disk space you have available. Specifically, you'll have to modify MAX_FILES, MAX_OCCUPIED_PERCENT, LOWER_THRESHOLD, and DISK_SPACE at the top of the file.

MAX_FILES helps you control how many files you want downloaded at once. I had around 400 GB available when I ran these scripts, so I chose 6 since 6 * 40 GB = 240 GB, which is reasonably less than 400 GB. Note that this doesn't mean that I'll at most be using 240 GB at a time, for two main reasons. First, as mentioned previously, we download files and process others in parallel. So I'd actually be using a bit more than 240 GB (files being downloaded + files being processed). Second, the process is not perfect: it's possible, but not necessary, that you download a bunch of files, but processing them is taking a long time, so you accumulate FASTQ files and you end up consuming more and more disk space. For this reason, the scripts will stop downloading files once the percent of disk space consumed reaches a certain percentage limit (that's what MAX_OCCUPIED_PERCENT represents). It'll then stop downloading and finish processing whatever it is currently processing, which should bring the disk space back down to a predefined threshold percentage. Once that lower threshold is reached again after exceeding the upper threshold and the necessary processing to go down again is done, the scripts will begin downloading more files and the process continues. That lower threshold percentage is defined by LOWER_THRESHOLD, and in our case is set to 30%. Again, you'll have to modify that to suit your specifics. What's important is that you set it to a number slightly larger than the percentage of disk space currently occupied on your machine; 5-10% higher should be a good place to start if you want to be really conservative. If you want to be precise, what you want to do is estimate the size needed to store the processed files. For our project, this was around 1.2GB. So if you had 200GB of disk space, then 1.2/200 = 0.5% is reasonable. Why is this important anyway? Because note that once you're done processing a batch of files, those processed files occupy disk space. So if you set your lower threshold to be the original percentage of disk space you began with when you began downloading, then you're never going to reach that threshold! You're always going to reach a percentage slightly higher. 

Finally, you have to change DISK_SPACE so that it reflects the disk space on your machine. For me, that was `^/dev/nvme0n1p2`. Just run `df -H` in the container and figure out which "Filesystem" represents your disk space, then change DISK_SPACE to reflect that. 

An important note: when setting the numbers for MAX_OCCUPIED_PERCENT and LOWER_THRESHOLD, make sure they're reasonably far from each other, and work well with the number of files you're downloading in a single batch. What I mean by this is the following: say that instead of setting them to 90 and 30, I set them to 35 and 30. When I begin downloading, I'm definitely going to exceed the 35% disk space threshold if I'm downloading 6 files at once (since my total disk space is 500GB and ((35-30)% * 500GB = 25GB < 240 GB). So I'll exceed the upper threshold, the downloading process stops, but the files haven't been completely downloaded yet. So the scripts will delete these temporary files, and we'll reach the lower threshold. They then attempt to download them again, but the exact same scenario repeats itself, and you're essentially stuck in an infinite loop. There are other quirky scenarios like this that might occur which I'm not going to go into, because they all happen for the same reason (you've set the threshold too close to each other, and/or you're downloading too many files at the same time in one batch). Moral of the story: make sure the thresholds are far apart and the amount of space consumed by the number of files in one batch fits comfortably within that percentage of disk space between them. This may or may not take some experimentation on your part, but it shouldn't be that bad. Also, nothing "bad" will happen if you encounter the previously mentioned scenario. Just notice it if it does happen, make adjustments to your thresholds / number of files downloaded in one batch accordingly, and run `./monitor_disk_space.sh` again. How to notice it? When the script stops downloading due to exceeding the upper threshold, it will echo the percentage of disk space currently occupied every few seconds. If you notice that this percentage is "stuck" at the same number for too long, then that means you need to make adjustments to the parameters. It's totally fine if this happens, just stop the script by running `CTRL + C`, and rerun `./monitor_disk_space.sh` after you've made your adjustments.

This brings me to my next point about these scripts. What's nice about them is that if anything "bad" happens (like the example mentioned above, or you lose your internet connection for example), you can just run `./monitor_disk_space.sh` again, and it'll just pick up where it left off automatically. Similarly, if you want to pause the process, just run `CTRL + C`, and you can start the process up again by running `./monitor_disk_space.sh`. 

Finally, the last thing you need to know about the scripts is what they output. They create a directory `download_dir` which acts as a temporary storage directory for FASTQ files. Once the files are downloaded, they are zipped, and moved to a directory `ready_for_analysis`. Once they get to `ready_for_analysis`, they are mapped to the reference transcriptome `Caenorhabditis_elegans.WBcel235.cdna.all.fa` using `kallisto`. Actually, `kallisto` uses an index file to map to the transcriptome, which is why the file `Caenorhabditis_elegans.WBcel235.cdna.all.fa.index` is created if it doesn't exist. Once the process is done, you should have the directory `ready_for_analysis` with all the raw read counts for each sample in each study. Note that we provide the `ready_for_analysis` directory as a zipped tar archive. 

PS: SRR10087139, SRR10087140, SRR10087141 are weird. Although they're supposed to be paired-end, when you download their SRA files, you only get 1 file. So I processed them manually as if they were single end. 

To combine the data into a single data structure, run `combine_data.R`

**3. Normalization**

Once we have raw read counts, the last step is to perform inter and intra sample normalization, which we do using [GeTMM](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2246-7). To do this, run `./getmm_normalize.R`. This will create the directory `ready_for_analysis`, and the files `sra_to_bioprojects.csv` and `getmm_gene_expression_data.csv` in that directory. These are the important files created by this script, but it will also create intermediate files `lengths.Rdata`, `bioprojects.Rdata`, and `combined_studies_raw_counts.Rdata` in `query_download_process_data`, and `adjusted_gene_expression.Rdata` in `ready_for_analysis`. 


Note that we exclude two studies - `PRJNA282784` and `PRJNA261420` - which had only one sample, since Combat-Seq cannot deal with less than 2 samples per study. We also exclude `PRJNA575569`, which had poor quality reads. In total, we end up with 1227 samples with 22113 genes per sample. 

to correct our data for batch effects, so that samples are comparable across experiments - we do this using [Combat-Seq](https://academic.oup.com/nargab/article/2/3/lqaa078/5909519)





